{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: transformers in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (4.38.0.dev0)\n",
      "Requirement already satisfied: bitsandbytes in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: accelerate in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: peft in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: wandb in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (0.16.3)\n",
      "Requirement already satisfied: loralib in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: ipywidgets in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (8.1.2)\n",
      "Requirement already satisfied: filelock in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: scipy in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from bitsandbytes) (1.12.0)\n",
      "Requirement already satisfied: psutil in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (3.1.41)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (1.40.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from wandb) (4.25.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: six>=1.4.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: decorator in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U datasets transformers bitsandbytes accelerate \\\n",
    "    peft wandb loralib ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"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\"\n",
    "project_name = \"review-generation-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import os, sys, json \n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Dataset\n",
    "from datasets import load_dataset, Dataset\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "# PEFT\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, PeftModel, get_peft_model, get_peft_model_state_dict, set_peft_model_state_dict\n",
    "# Trainer\n",
    "from trl import SFTTrainer # supervised finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']='max_split_size_mb:512'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits and Bytes Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # loading in 4 bit\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=\"codellama/CodeLlama-7b-hf\"\n",
    "new_model = \"asif-train-500e-all-codellama-7b-ft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850e7ed85ea64cada0bd60bb9a3a7798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, quantization_config=bnb_config, device_map=\"auto\" # or {\"\": 0} or multi gpu?\n",
    ")\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the roberta/codereviewer tokenizer?\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "# or unk_token?\n",
    "tokenizer.pad_token = tokenizer.eos_token # there is no padding token for llama\n",
    "tokenizer.padding_side = \"right\" # or left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 306, 529, 17462, 29958], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I <keep>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tokenizer(\"I <keep>\")\n",
    "print(inp)\n",
    "tokenizer.decode(inp[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for modules: https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models\n",
    "peft_config = LoraConfig(\n",
    "    r=16, # rank, 8\n",
    "    lora_alpha=32, # strength of the adapter, impact on model, 16\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    # more modules, more training, better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_path = \"Comment_Generation/msg-train.jsonl\"\n",
    "val_data_file_path = \"Comment_Generation/msg-valid.jsonl\"\n",
    "test_data_file_path = \"Comment_Generation/msg-test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diff(diff):\n",
    "    difflines = diff.split(\"\\n\")[1:]\n",
    "    difflines = [line for line in difflines if len(line.strip()) > 0]\n",
    "    map_dic = {\"-\": 0, \"+\": 1, \" \": 2}\n",
    "    def f(s):\n",
    "        if s in map_dic:\n",
    "            return map_dic[s]\n",
    "        else:\n",
    "            return 2\n",
    "    labels = [f(line[0]) for line in difflines]\n",
    "    difflines = [line[1:].strip() for line in difflines]\n",
    "    inputstr = \"\"\n",
    "    for label, line in zip(labels, difflines):\n",
    "        if label == 1:\n",
    "            inputstr += \"<add>\" + line\n",
    "        elif label == 0:\n",
    "            inputstr += \"<del>\" + line\n",
    "        else:\n",
    "            inputstr += \"<keep>\" + line\n",
    "\n",
    "    return inputstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<keep>model.getFiles().stream().map(ProtoFile::getProto).collect(Collectors.toList()))<keep>// Only the file to generate a client for (don\\'t generate dependencies)<keep>.addFileToGenerate(\"multiple_services.proto\")<del>.setParameter(\"language=java\")<add>.setParameter(\"language=java,transport=grpc\")<keep>.build();<keep>CodeGeneratorResponse response = ProtocGeneratorMain.generate(codeGeneratorRequest);'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_data_file_path, 'r') as f:\n",
    "    line = f.readline()\n",
    "    data = json.loads(line)\n",
    "data = process_diff(data['patch'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>model.getFiles().stream().map(ProtoFile::getProto).collect(Collectors.toList()))<keep>// Only the file to generate a client for (don't generate dependencies)<keep>.addFileToGenerate(\"multiple_services.proto\")<del>.setParameter(\"language=java\")<add>.setParameter(\"language=java,transport=grpc\")<keep>.build();<keep>CodeGeneratorResponse response = ProtocGeneratorMain.generate(codeGeneratorRequest);\n",
      "\n",
      "### Review Comment:\n",
      "<keep>The code change is to add a parameter to the code generation request. The parameter is \"language=java,transport=grpc\".<keep>\n",
      "\n",
      "### Hint:\n",
      "<keep>The code change is a diff\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"\"\"You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
    "\n",
    "### Question:\n",
    "What would be your suggested review comment?\n",
    "\n",
    "### Code Change:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "\n",
    "### Review Comment:\n",
    "\"\"\"\n",
    "prompt = prompt1 + str(data) + prompt2\n",
    "# print(prompt)\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**batch, max_new_tokens=50)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                js = json.loads(line.strip())\n",
    "            except:\n",
    "                print(\"Error during reading json data.\")\n",
    "                continue\n",
    "            data.append(js)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_jsonl(train_data_file_path)\n",
    "val_data = read_jsonl(val_data_file_path)\n",
    "test_data = read_jsonl(test_data_file_path)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i][\"idx\"] = i\n",
    "for i in range(len(val_data)):\n",
    "    val_data[i][\"idx\"] = i\n",
    "\n",
    "for dic in train_data:\n",
    "    diff, msg = dic[\"patch\"], dic[\"msg\"]\n",
    "    dic[\"diff\"] = process_diff(diff)\n",
    "for dic in val_data:\n",
    "    diff, msg = dic[\"patch\"], dic[\"msg\"]\n",
    "    dic[\"diff\"] = process_diff(diff)\n",
    "for dic in test_data:\n",
    "    diff, msg = dic[\"patch\"], dic[\"msg\"]\n",
    "    dic[\"diff\"] = process_diff(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;keep&gt;array_1d&lt;double, 3&gt; b = ZeroVector(3);&lt;k...</td>\n",
       "      <td>I assumed that for CrossProduct the values wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;keep&gt;For internal use only; no backwards-comp...</td>\n",
       "      <td>I think we should we avoid `import six` for co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;keep&gt;def should_render_revenue?&lt;keep&gt;revenue ...</td>\n",
       "      <td>we call cities + towns . size a lot, maybe mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;keep&gt;D_ERROR(\"pool \"DF_UUID\" event %d failed:...</td>\n",
       "      <td>This will be removed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;keep&gt;return middleware.Wrap(h, backendMiddlew...</td>\n",
       "      <td>nit: `firehoseLogHandler` vs. `firehoseMiddlew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117734</th>\n",
       "      <td>&lt;keep&gt;########################################...</td>\n",
       "      <td>For variants like this, you should _explicitly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117735</th>\n",
       "      <td>&lt;keep&gt;if user &amp;&amp; user.student?&lt;keep&gt;if user.hi...</td>\n",
       "      <td>Align the parameters of a method call if they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117736</th>\n",
       "      <td>&lt;keep&gt;profit_trade = trade.calc_profit(rate=pr...</td>\n",
       "      <td>Below this - we'll need to make sure to use th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117737</th>\n",
       "      <td>&lt;keep&gt;return &amp;evt, nil&lt;keep&gt;}&lt;del&gt;func populat...</td>\n",
       "      <td>I understand this new parameter is unused, may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117738</th>\n",
       "      <td>&lt;keep&gt;var docUri = new Uri(document.location.h...</td>\n",
       "      <td>Instead of passing a second argument to `then`...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117739 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     diff  \\\n",
       "0       <keep>array_1d<double, 3> b = ZeroVector(3);<k...   \n",
       "1       <keep>For internal use only; no backwards-comp...   \n",
       "2       <keep>def should_render_revenue?<keep>revenue ...   \n",
       "3       <keep>D_ERROR(\"pool \"DF_UUID\" event %d failed:...   \n",
       "4       <keep>return middleware.Wrap(h, backendMiddlew...   \n",
       "...                                                   ...   \n",
       "117734  <keep>########################################...   \n",
       "117735  <keep>if user && user.student?<keep>if user.hi...   \n",
       "117736  <keep>profit_trade = trade.calc_profit(rate=pr...   \n",
       "117737  <keep>return &evt, nil<keep>}<del>func populat...   \n",
       "117738  <keep>var docUri = new Uri(document.location.h...   \n",
       "\n",
       "                                                      msg  \n",
       "0       I assumed that for CrossProduct the values wer...  \n",
       "1       I think we should we avoid `import six` for co...  \n",
       "2       we call cities + towns . size a lot, maybe mak...  \n",
       "3                                   This will be removed.  \n",
       "4       nit: `firehoseLogHandler` vs. `firehoseMiddlew...  \n",
       "...                                                   ...  \n",
       "117734  For variants like this, you should _explicitly...  \n",
       "117735  Align the parameters of a method call if they ...  \n",
       "117736  Below this - we'll need to make sure to use th...  \n",
       "117737  I understand this new parameter is unused, may...  \n",
       "117738  Instead of passing a second argument to `then`...  \n",
       "\n",
       "[117739 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data into pandas dataframe in two columns named \"diff\" and \"msg\"\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df = train_df[[\"diff\", \"msg\"]]\n",
    "# df.iloc[0][\"diff\"]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;keep&gt;)&lt;keep&gt;return rv&lt;add&gt;@app.template_test(...</td>\n",
       "      <td>Should we call it `is_list`?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;keep&gt;configureSqlClientInstrumentationOptions...</td>\n",
       "      <td>in the instrumentation example, should we use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;keep&gt;## Fields ##&lt;keep&gt;############&lt;del&gt;class...</td>\n",
       "      <td>Why this change ? Is it useful ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;add&gt;const titleNode = virtualNode.children.fi...</td>\n",
       "      <td>I know this is a nitpick, but don't we always ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;keep&gt;assertEquals(false, EMailValidator.isEma...</td>\n",
       "      <td>We should reformat this emails in the test to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10314</th>\n",
       "      <td>&lt;keep&gt;dropped = [dims.index(d) for d in dims&lt;k...</td>\n",
       "      <td>So calling `np.squeeze` will probably make thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315</th>\n",
       "      <td>&lt;keep&gt;assert frame1.stypes == frame2.stypes, (...</td>\n",
       "      <td>Won't this produce too much of output if frame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10316</th>\n",
       "      <td>&lt;keep&gt;)&lt;keep&gt;type NoopProvider struct{}&lt;del&gt;ty...</td>\n",
       "      <td>This seems like a hold-over from the interface...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>&lt;keep&gt;}&lt;keep&gt;if (GetLevel() &gt;= item-&gt;Click.Lev...</td>\n",
       "      <td>Do we not want a message here similar to the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10318</th>\n",
       "      <td>&lt;keep&gt;...bgColor.propTypes,&lt;keep&gt;}&lt;add&gt;export ...</td>\n",
       "      <td>I think the shorthand should be removed; the e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10319 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    diff  \\\n",
       "0      <keep>)<keep>return rv<add>@app.template_test(...   \n",
       "1      <keep>configureSqlClientInstrumentationOptions...   \n",
       "2      <keep>## Fields ##<keep>############<del>class...   \n",
       "3      <add>const titleNode = virtualNode.children.fi...   \n",
       "4      <keep>assertEquals(false, EMailValidator.isEma...   \n",
       "...                                                  ...   \n",
       "10314  <keep>dropped = [dims.index(d) for d in dims<k...   \n",
       "10315  <keep>assert frame1.stypes == frame2.stypes, (...   \n",
       "10316  <keep>)<keep>type NoopProvider struct{}<del>ty...   \n",
       "10317  <keep>}<keep>if (GetLevel() >= item->Click.Lev...   \n",
       "10318  <keep>...bgColor.propTypes,<keep>}<add>export ...   \n",
       "\n",
       "                                                     msg  \n",
       "0                           Should we call it `is_list`?  \n",
       "1      in the instrumentation example, should we use ...  \n",
       "2                       Why this change ? Is it useful ?  \n",
       "3      I know this is a nitpick, but don't we always ...  \n",
       "4      We should reformat this emails in the test to ...  \n",
       "...                                                  ...  \n",
       "10314  So calling `np.squeeze` will probably make thi...  \n",
       "10315  Won't this produce too much of output if frame...  \n",
       "10316  This seems like a hold-over from the interface...  \n",
       "10317  Do we not want a message here similar to the o...  \n",
       "10318  I think the shorthand should be removed; the e...  \n",
       "\n",
       "[10319 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data into pandas dataframe in two columns named \"diff\" and \"msg\"\n",
    "import pandas as pd\n",
    "val_df = pd.DataFrame(val_data)\n",
    "val_df = val_df[[\"diff\", \"msg\"]]\n",
    "# df.iloc[0][\"diff\"]\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;keep&gt;model.getFiles().stream().map(ProtoFile:...</td>\n",
       "      <td>can we also test for `transport=rest`?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;keep&gt;*/&lt;keep&gt;protected function createBackend...</td>\n",
       "      <td>If record_batch_size is not set in config.ini,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;keep&gt;&lt;script type=\"text/javascript\"&gt;&lt;keep&gt;win...</td>\n",
       "      <td>I didn't realize we were hardcoding this, than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;keep&gt;}&lt;keep&gt;&lt;/h4&gt;&lt;keep&gt;&lt;div class=\"UppyDashbo...</td>\n",
       "      <td>We are trying to support IE 10-11, so we'll ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;keep&gt;function ResetButton( { children } ) {&lt;k...</td>\n",
       "      <td>It looks like there's a new `isNavigatingTo( u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10164</th>\n",
       "      <td>&lt;keep&gt;type ConsumerConfig struct {&lt;keep&gt;Public...</td>\n",
       "      <td>Should be from lowercase `json:\"ports\"` the sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10165</th>\n",
       "      <td>&lt;keep&gt;# Purpose:&lt;keep&gt;# sns-ruby-example-creat...</td>\n",
       "      <td>Simple Notification **Service** (singular)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10166</th>\n",
       "      <td>&lt;keep&gt;@Override&lt;keep&gt;public void runScript(Str...</td>\n",
       "      <td>This code should be executed in NashornEngineF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10167</th>\n",
       "      <td>&lt;keep&gt;// NewTags creates a tags object&lt;keep&gt;fu...</td>\n",
       "      <td>this is not so great as it sets the global ran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10168</th>\n",
       "      <td>&lt;keep&gt;arity = \"1\")&lt;keep&gt;private final Wei txFe...</td>\n",
       "      <td>This looks inverted to me: if `--rpc-require-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    diff  \\\n",
       "0      <keep>model.getFiles().stream().map(ProtoFile:...   \n",
       "1      <keep>*/<keep>protected function createBackend...   \n",
       "2      <keep><script type=\"text/javascript\"><keep>win...   \n",
       "3      <keep>}<keep></h4><keep><div class=\"UppyDashbo...   \n",
       "4      <keep>function ResetButton( { children } ) {<k...   \n",
       "...                                                  ...   \n",
       "10164  <keep>type ConsumerConfig struct {<keep>Public...   \n",
       "10165  <keep># Purpose:<keep># sns-ruby-example-creat...   \n",
       "10166  <keep>@Override<keep>public void runScript(Str...   \n",
       "10167  <keep>// NewTags creates a tags object<keep>fu...   \n",
       "10168  <keep>arity = \"1\")<keep>private final Wei txFe...   \n",
       "\n",
       "                                                     msg  \n",
       "0                 can we also test for `transport=rest`?  \n",
       "1      If record_batch_size is not set in config.ini,...  \n",
       "2      I didn't realize we were hardcoding this, than...  \n",
       "3      We are trying to support IE 10-11, so we'll ne...  \n",
       "4      It looks like there's a new `isNavigatingTo( u...  \n",
       "...                                                  ...  \n",
       "10164  Should be from lowercase `json:\"ports\"` the sa...  \n",
       "10165         Simple Notification **Service** (singular)  \n",
       "10166  This code should be executed in NashornEngineF...  \n",
       "10167  this is not so great as it sets the global ran...  \n",
       "10168  This looks inverted to me: if `--rpc-require-c...  \n",
       "\n",
       "[10169 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data into pandas dataframe in two columns named \"diff\" and \"msg\"\n",
    "import pandas as pd\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df = test_df[[\"diff\", \"msg\"]]\n",
    "# df.iloc[0][\"diff\"]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['diff', 'msg'],\n",
       "    num_rows: 117739\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "# final_data = final_data.map(lambda samples: tokenizer(samples['diff']), batched=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['diff', 'msg'],\n",
       "    num_rows: 10319\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "# final_data = final_data.map(lambda samples: tokenizer(samples['diff']), batched=True)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['diff', 'msg'],\n",
       "    num_rows: 10169\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "# final_data = final_data.map(lambda samples: tokenizer(samples['diff']), batched=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompt1 + str(process_diff(data_point['diff'])) + prompt2 + str(data_point['msg'])\n",
    "    return tokenize(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df65c18eeebe4ce6865f93768e5d76aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb899f04f9d46628135a2f69a88ef28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_set = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_set = val_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['diff', 'msg', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 117739\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['diff', 'msg', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10319\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_from_checkpoint = \"asif-train-200e-all-codellama-7b-ft/adapter_model.bin\" # set this to the adapter_model.bin file you want to resume from\n",
    "\n",
    "# if resume_from_checkpoint:\n",
    "#     if os.path.exists(resume_from_checkpoint):\n",
    "#         print(f\"Restarting from {resume_from_checkpoint}\")\n",
    "#         adapters_weights = torch.load(resume_from_checkpoint)\n",
    "#         set_peft_model_state_dict(model, adapters_weights)\n",
    "#     else:\n",
    "#         print(f\"Checkpoint {resume_from_checkpoint} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project = project_name\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "per_device_train_batch_size = 16\n",
    "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
    "output_dir = project_name\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        max_steps=500,\n",
    "        learning_rate=3e-4,\n",
    "        fp16=True, # ?\n",
    "        logging_steps=10,\n",
    "        lr_scheduler_type=\"linear\", # ?\n",
    "        optim=\"adamw_torch\", # ?\n",
    "        evaluation_strategy=\"steps\", # if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_steps=150,\n",
    "        output_dir=output_dir,\n",
    "        # save_total_limit=3,\n",
    "        load_best_model_at_end=False,\n",
    "        # ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=True, # group sequences of roughly the same length together to speed up training\n",
    "        report_to=\"wandb\", # if use_wandb else \"none\",\n",
    "        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n",
    "    )\n",
    "model = get_peft_model(model, peft_config)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_set,\n",
    "    eval_dataset=tokenized_val_set,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq( # ?\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "    model, type(model)\n",
    ")\n",
    "\n",
    "# Red Flag\n",
    "# if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "#     print(\"compiling the model\")\n",
    "#     model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33m1805112\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/quadro/NVME/Asif-Thesis/Llama/wandb/run-20240209_063030-satoom92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/1805112/review-generation-v2/runs/satoom92' target=\"_blank\">codellama-2024-02-09-06-30</a></strong> to <a href='https://wandb.ai/1805112/review-generation-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/1805112/review-generation-v2' target=\"_blank\">https://wandb.ai/1805112/review-generation-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/1805112/review-generation-v2/runs/satoom92' target=\"_blank\">https://wandb.ai/1805112/review-generation-v2/runs/satoom92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fc5a62e4f3460185b37baac4450ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0338, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5548, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.01}\n",
      "{'loss': 1.1353, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6034, 'learning_rate': 0.00011999999999999999, 'epoch': 0.02}\n",
      "{'loss': 0.3864, 'learning_rate': 0.00015, 'epoch': 0.03}\n",
      "{'loss': 1.2878, 'learning_rate': 0.00017999999999999998, 'epoch': 0.03}\n",
      "{'loss': 0.8884, 'learning_rate': 0.00020999999999999998, 'epoch': 0.04}\n",
      "{'loss': 0.666, 'learning_rate': 0.00023999999999999998, 'epoch': 0.04}\n",
      "{'loss': 0.4952, 'learning_rate': 0.00027, 'epoch': 0.05}\n",
      "{'loss': 0.3047, 'learning_rate': 0.0003, 'epoch': 0.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70491619303a4abcb10db2c0ad246a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.761552631855011, 'eval_runtime': 1133.418, 'eval_samples_per_second': 9.104, 'eval_steps_per_second': 1.138, 'epoch': 0.05}\n",
      "{'loss': 1.2278, 'learning_rate': 0.00029249999999999995, 'epoch': 0.06}\n",
      "{'loss': 0.8676, 'learning_rate': 0.000285, 'epoch': 0.07}\n",
      "{'loss': 0.6472, 'learning_rate': 0.00027749999999999997, 'epoch': 0.07}\n",
      "{'loss': 0.4963, 'learning_rate': 0.00027, 'epoch': 0.08}\n",
      "{'loss': 0.2916, 'learning_rate': 0.0002625, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.233, 'learning_rate': 0.00025499999999999996, 'epoch': 0.09}\n",
      "{'loss': 0.8345, 'learning_rate': 0.00024749999999999994, 'epoch': 0.09}\n",
      "{'loss': 0.6433, 'learning_rate': 0.00023999999999999998, 'epoch': 0.1}\n",
      "{'loss': 0.461, 'learning_rate': 0.00023249999999999999, 'epoch': 0.1}\n",
      "{'loss': 0.2924, 'learning_rate': 0.000225, 'epoch': 0.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1813752d89a470787ccd2365bc4b5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7577228546142578, 'eval_runtime': 1135.7529, 'eval_samples_per_second': 9.086, 'eval_steps_per_second': 1.136, 'epoch': 0.11}\n",
      "{'loss': 1.2154, 'learning_rate': 0.00021749999999999997, 'epoch': 0.11}\n",
      "{'loss': 0.8466, 'learning_rate': 0.00020999999999999998, 'epoch': 0.12}\n",
      "{'loss': 0.6436, 'learning_rate': 0.0002025, 'epoch': 0.13}\n",
      "{'loss': 0.4784, 'learning_rate': 0.000195, 'epoch': 0.13}\n",
      "{'loss': 0.3019, 'learning_rate': 0.00018749999999999998, 'epoch': 0.14}\n",
      "{'loss': 1.2192, 'learning_rate': 0.00017999999999999998, 'epoch': 0.14}\n",
      "{'loss': 0.8577, 'learning_rate': 0.00017249999999999996, 'epoch': 0.15}\n",
      "{'loss': 0.6565, 'learning_rate': 0.000165, 'epoch': 0.15}\n",
      "{'loss': 0.472, 'learning_rate': 0.00015749999999999998, 'epoch': 0.16}\n",
      "{'loss': 0.2943, 'learning_rate': 0.00015, 'epoch': 0.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7ceb915139468bb2083d97936cb851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7403162121772766, 'eval_runtime': 1135.6107, 'eval_samples_per_second': 9.087, 'eval_steps_per_second': 1.136, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2152, 'learning_rate': 0.0001425, 'epoch': 0.17}\n",
      "{'loss': 0.8361, 'learning_rate': 0.000135, 'epoch': 0.17}\n",
      "{'loss': 0.6331, 'learning_rate': 0.00012749999999999998, 'epoch': 0.18}\n",
      "{'loss': 0.4757, 'learning_rate': 0.00011999999999999999, 'epoch': 0.18}\n",
      "{'loss': 0.3057, 'learning_rate': 0.0001125, 'epoch': 0.19}\n",
      "{'loss': 1.2026, 'learning_rate': 0.00010499999999999999, 'epoch': 0.2}\n",
      "{'loss': 0.8404, 'learning_rate': 9.75e-05, 'epoch': 0.2}\n",
      "{'loss': 0.642, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.21}\n",
      "{'loss': 0.467, 'learning_rate': 8.25e-05, 'epoch': 0.21}\n",
      "{'loss': 0.2959, 'learning_rate': 7.5e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc923bfa6a14e14a9549d947f337a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7357637286186218, 'eval_runtime': 1133.656, 'eval_samples_per_second': 9.102, 'eval_steps_per_second': 1.138, 'epoch': 0.22}\n",
      "{'loss': 1.1983, 'learning_rate': 6.75e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8557, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6342, 'learning_rate': 5.2499999999999995e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4722, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3063, 'learning_rate': 3.75e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2004, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.25}\n",
      "{'loss': 0.8332, 'learning_rate': 2.2499999999999998e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6374, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4794, 'learning_rate': 7.499999999999999e-06, 'epoch': 0.27}\n",
      "{'loss': 0.2996, 'learning_rate': 0.0, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b707b664ac754e32bffcd973a0f002b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7340313196182251, 'eval_runtime': 1134.2305, 'eval_samples_per_second': 9.098, 'eval_steps_per_second': 1.137, 'epoch': 0.27}\n",
      "{'train_runtime': 12871.2706, 'train_samples_per_second': 2.486, 'train_steps_per_second': 0.039, 'train_loss': 0.7833351621627808, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.7833351621627808, metrics={'train_runtime': 12871.2706, 'train_samples_per_second': 2.486, 'train_steps_per_second': 0.039, 'train_loss': 0.7833351621627808, 'epoch': 0.27})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainOutput(global_step=200, training_loss=0.8567268979549408, metrics={'train_runtime': 7423.7516, \n",
    "# 'train_samples_per_second': 1.724, 'train_steps_per_second': 0.027, 'train_loss': 0.8567268979549408, 'epoch': 0.11})\n",
    "\n",
    "# TrainOutput(global_step=500, training_loss=0.7833351621627808, metrics={'train_runtime': 12871.2706, \n",
    "# 'train_samples_per_second': 2.486, 'train_steps_per_second': 0.039, 'train_loss': 0.7833351621627808, 'epoch': 0.27})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/27397\n",
    "trainer.model.save_pretrained(new_model, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "# del pipe\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location = \"asifhaider/\" + new_model\n",
    "# model.push_to_hub(location, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5966595e101a453990454d0e90a90b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "   quantization_config=bnb_config, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:249: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# output_dir = output_dir + \"/checkpoint-200\"\n",
    "output_dir = \"asif-train-500e-all-codellama-7b-ft\"\n",
    "model = PeftModel.from_pretrained(model, output_dir)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I didn't realize we were hardcoding this, thanks for moving it to an env value.\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[2][\"msg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = prompt1 + test_df.iloc[2][\"diff\"] + prompt2\n",
    "# print(eval_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep><script type=\"text/javascript\"><keep>window.analytics||(window.analytics=[]),window.analytics.methods=[\"identify\",\"track\",\"trackLink\",\"trackForm\",\"trackClick\",\"trackSubmit\",\"page\",\"pageview\",\"ab\",\"alias\",\"ready\",\"group\",\"on\",\"once\",\"off\"],window.analytics.factory=function(t){return function(){var a=Array.prototype.slice.call(arguments);return a.unshift(t),window.analytics.push(a),window.analytics}};for(var i=0;i<window.analytics.methods.length;i++){var method=window.analytics.methods[i];window.analytics[method]=window.analytics.factory(method)}window.analytics.load=function(t){var a=document.createElement(\"script\");a.type=\"text/javascript\",a.async=!0,a.src=(\"https:\"===document.location.protocol?\"https://\":\"http://\")+\"d2dq2ahtl5zl1z.cloudfront.net/analytics.js/v1/\"+t+\"/analytics.min.js\";var n=document.getElementsByTagName(\"script\")[0];n.parentNode.insertBefore(a,n)},window.analytics.SNIPPET_VERSION=\"2.0.8\",<del>window.analytics.load(\"2nexpdgku3\");<add>window.analytics.load(<%= ENV['SEGMENT_KEY']%>);<keep>window.analytics.page();<keep></script>\n",
      "\n",
      "### Review Comment:\n",
      "<keep><script type=\"text/javascript\"><keep>window.analytics||(window.analytics=[]),window.analytics.methods=[\"identify\",\"track\",\"trackLink\",\"trackForm\",\"trackClick\",\"trackSubmit\",\"page\",\"pageview\",\"ab\",\"alias\",\"ready\",\"group\",\"on\",\"once\",\"off\"],window\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=70)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={\"\":0},\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt = \"'<keep>*/<keep>public CoreDescriptor(String name, Path instanceDir, Map<String, String> coreProps,<keep>Properties containerProperties, ZkController zkController) {<del>this.instanceDir = instanceDir;<add>this.instanceDir = instanceDir.toAbsolutePath();<keep>originalCoreProperties.setProperty(CORE_NAME, name);'\"\n",
    "# # instruction = f\"### [Patch For Review]:\\n{prompt}\\n\\n### [Review Comment]:\\n\"\n",
    "# sequences = pipe(eval_prompt,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.2,\n",
    "#     top_p=0.9,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     max_length=128,\n",
    "#     truncation=True\n",
    "# )\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>model.getFiles().stream().map(ProtoFile::getProto).collect(Collectors.toList()))<keep>// Only the file to generate a client for (don't generate dependencies)<keep>.addFileToGenerate(\"multiple_services.proto\")<del>.setParameter(\"language=java\")<add>.setParameter(\"language=java,transport=grpc\")<keep>.build();<keep>CodeGeneratorResponse response = ProtocGeneratorMain.generate(codeGeneratorRequest);\n",
      "\n",
      "### Review Comment:\n",
      "<keep>The code change is to add a parameter to the code generation request. The parameter is \"language=java,transport=grpc\".<keep>\n",
      "\n",
      "### Hint:\n",
      "<keep>The code change is a diff hunk. You can use the diff hunk to find the code change.\n",
      "\n",
      "###\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>*/<keep>protected function createBackend(Connector $connector)<keep>{<add>$config = $this->config->get($this->mainConfig);<keep>$backend = new $this->backendClass($connector);<add>$backend->setPageSize($config->Index->record_batch_size);<keep>$backend->setQueryBuilder($this->createQueryBuilder());<keep>$backend->setSimilarBuilder($this->createSimilarBuilder());<keep>if ($this->logger) {\n",
      "\n",
      "### Review Comment:\n",
      "<keep>*/<keep>protected function createBackend(Connector $connector)<keep>{<add>$config = $this->config->get($this->mainConfig);<keep>$backend = new $this->backendClass($connector);<add>$backend->setPageSize($config->Index->\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep><script type=\"text/javascript\"><keep>window.analytics||(window.analytics=[]),window.analytics.methods=[\"identify\",\"track\",\"trackLink\",\"trackForm\",\"trackClick\",\"trackSubmit\",\"page\",\"pageview\",\"ab\",\"alias\",\"ready\",\"group\",\"on\",\"once\",\"off\"],window.analytics.factory=function(t){return function(){var a=Array.prototype.slice.call(arguments);return a.unshift(t),window.analytics.push(a),window.analytics}};for(var i=0;i<window.analytics.methods.length;i++){var method=window.analytics.methods[i];window.analytics[method]=window.analytics.factory(method)}window.analytics.load=function(t){var a=document.createElement(\"script\");a.type=\"text/javascript\",a.async=!0,a.src=(\"https:\"===document.location.protocol?\"https://\":\"http://\")+\"d2dq2ahtl5zl1z.cloudfront.net/analytics.js/v1/\"+t+\"/analytics.min.js\";var n=document.getElementsByTagName(\"script\")[0];n.parentNode.insertBefore(a,n)},window.analytics.SNIPPET_VERSION=\"2.0.8\",<del>window.analytics.load(\"2nexpdgku3\");<add>window.analytics.load(<%= ENV['SEGMENT_KEY']%>);<keep>window.analytics.page();<keep></script>\n",
      "\n",
      "### Review Comment:\n",
      "<keep><script type=\"text/javascript\"><keep>window.analytics||(window.analytics=[]),window.analytics.methods=[\"identify\",\"track\",\"trackLink\",\"trackForm\",\"trackClick\",\"trackSubmit\",\"page\",\"pageview\",\"ab\",\"alias\",\"ready\",\"group\",\"on\",\"once\",\"off\"],window\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>}<keep></h4><keep><div class=\"UppyDashboardItem-status\"><del>${file.data.size && html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`}<add>${isNaN(file.data.size) ? '' : html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`}<keep>${file.source && html`<div class=\"UppyDashboardItem-sourceIcon\"><keep>${acquirers.map(acquirer => {<keep>if (acquirer.id === file.source) return html`<span title=\"${props.i18n('fileSource')}: ${acquirer.name}\">${acquirer.icon()}</span>`\n",
      "\n",
      "### Review Comment:\n",
      "<keep>${props.i18n('fileSource')}: ${acquirer.name}</keep>\n",
      "\n",
      "### Explanation:\n",
      "You are given a code change, and context. You are asked to suggest a review comment. You are given a code change, and context. You are asked to suggest a review\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>function ResetButton( { children } ) {<keep>const postResetURL = useSelect( ( select ) => select( CORE_SITE ).getAdminURL( 'googlesitekit-splash', { notification: 'reset_success' } ) );<add>const isNavigating = useSelect( ( select ) => select( CORE_LOCATION ).isNavigating() );<add>const navigatingURL = useSelect( ( select ) => select( CORE_LOCATION ).getNavigateURL() );<add><keep>const [ dialogActive, setDialogActive ] = useState( false );<keep>useEffect( () => {\n",
      "\n",
      "### Review Comment:\n",
      "<keep>* The code is not clear.\n",
      "<keep>* The code is not clear.\n",
      "<keep>* The code is not clear.\n",
      "<keep>* The code is not clear.\n",
      "<keep>* The code is not clear.\n",
      "<keep>* The code is not clear.\n",
      "<keep>*\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>namespace lbann {<del>void im2col(const Mat& im,<del>Mat& col,<add>void im2col(const AbsMat& im,<add>AbsMat& col,<keep>const int num_channels,<keep>const int im_num_dims,<keep>const int * im_dims,\n",
      "\n",
      "### Review Comment:\n",
      "<keep>// Check if the dimensions of the image are valid<del>// Check if the dimensions of the image are valid<del>// Check if the dimensions of the image are valid<keep>if (im_num_dims != 3) {<keep>  std::stringstream err;<keep>  err\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>from google.cloud.forseti.notifier.notifiers.inventory_summary import InventorySummary<keep>from google.cloud.forseti.services.inventory.storage import DataAccess<keep>from google.cloud.forseti.services.scanner import dao as scanner_dao<add>from google.cloud.forseti.common.util.email.email_factory import EmailFactory<add>from google.cloud.forseti.notifier.notifiers import email_violations<keep># pylint: enable=line-too-long\n",
      "\n",
      "### Review Comment:\n",
      "<add>The code change is to add a new function to the InventorySummary class. The function is called send_email_violations and it is used to send email notifications to users.\n",
      "\n",
      "### Code Change:\n",
      "<keep>from google.cloud.forseti.notifier.notifiers.inventory_\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>if ( rasCmdLine->function >0 )<keep>function = rasCmdLine->bus;<del>snprintf(sysfs_path, sizeof(sysfs_path),<add>snprintf_s_iiii(sysfs_path, sizeof(sysfs_path),<keep>DEVICEID_PATH,0,bus,device,function);<keep>result = sysfs_read_u64(sysfs_path, &value);\n",
      "\n",
      "### Review Comment:\n",
      "<keep>if ( rasCmdLine->function >0 )<keep>function = rasCmdLine->bus;<del>snprintf(sysfs_path, sizeof(sysfs_path),<add>snprintf_s_iiii(sysfs_path, sizeof(sysfs_path),<keep>DEVICE\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>for k in range(256):<keep>keyStates[k]=ctypes.windll.user32.GetKeyState(k)<keep>charBuf=ctypes.create_unicode_buffer(5)<add># First try getting the keyboard layout from the thread with the focus (input thread)<keep>hkl=ctypes.windll.user32.GetKeyboardLayout(focus.windowThreadID)<add>if not hkl:<add>log.debug(\"Failed to fetch keyboard layout from focus, trying layout from last detected change\")<add># Some threads, such as for Windows consoles<add># Do not allow getKeyboardLayout to work.<add># Therefore, use the cached keyboard layout from the last inputLangChange detected by NVDA<add># on the foreground object.<add>hkl = getattr(api.getForegroundObject(), '_lastDetectedKeyboardLayoutChange', 0)<add>if not hkl:<add>log.debug(\"No layout cached, falling back to layout of NVDA main thread\")<add># As a last resort, use the keyboard layout of NVDA's main thread.<add>hkl = ctypes.windll.user32.GetKeyboardLayout(core.mainThreadId)<keep># In previous Windows builds, calling ToUnicodeEx would destroy keyboard buffer state and therefore cause the app to not produce the right WM_CHAR message.<keep># However, ToUnicodeEx now can take a new flag of 0x4, which stops it from destroying keyboard state, thus allowing us to safely call it here.<keep>res=ctypes.windll.user32.ToUnicodeEx(vkCode,scanCode,keyStates,charBuf,len(charBuf),0x4,hkl)\n",
      "\n",
      "### Review Comment:\n",
      "The code change is a call to the Windows API function ToUnicodeEx. The function is used to convert a virtual key code to a Unicode character. The function is used to convert a virtual key code to a Unicode character. The function is used to convert a virtual key code to a Unicode character. The function is used to convert a virtual key\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>* @return {!Driver} A new driver instance.<keep>*/<keep>static createSession(options, service = getDefaultService()) {<add>if (!service) {<add>service = getDefaultService();<add>}<add><keep>let client = service.start().then(url => new http.HttpClient(url));<keep>let executor = new http.Executor(client);\n",
      "\n",
      "### Review Comment:\n",
      "<add>The service is not used in this method.\n",
      "\n",
      "### Hint:\n",
      "<add>You can use the <add>*<add> to mark a line as a comment.\n",
      "\n",
      "### Example:\n",
      "<add>* @return {!Driver} A new driver instance.<add>*/<add>\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>if (!this.driver_.fileDetector_) {<keep>return this.schedule_(<keep>new command.Command(command.Name.SEND_KEYS_TO_ELEMENT).<del>setParameter('text', keys).<add>setParameter('text', keys.then(keys => keys.join(''))).<keep>setParameter('value', keys),<keep>'WebElement.sendKeys()');<keep>}\n",
      "\n",
      "### Review Comment:\n",
      "<keep>if (!this.driver_.fileDetector_) {<keep>return this.schedule_(<keep>new command.Command(command.Name.SEND_KEYS_TO_ELEMENT).<del>setParameter('text', keys).<add>setParameter('text', keys.then(keys\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>name := opts.PVName<keep>stgType := volumeConfig.GetStorageType()<keep>saName := getOpenEBSServiceAccountName()<add>shared := volumeConfig.GetSharedMountValue()<keep>path, err := volumeConfig.GetPath()<keep>if err != nil {\n",
      "\n",
      "### Review Comment:\n",
      "<add>The shared mount value is not set in the volumeConfig.\n",
      "\n",
      "### Hint:\n",
      "<add>The shared mount value is not set in the volumeConfig.\n",
      "\n",
      "### Solution:\n",
      "<add>The shared mount value is not set in the volumeConfig.\n",
      "\n",
      "### Code Change:\n",
      "<keep\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>}<keep>/**<del>* Freezes the given column (add it to fixed columns).<add>* Freezes the specified column (i.e. adds it to fixed columns).<add>*<add>* `freezeColumn()` doesn't re-render the table,<add>* so you need to call the `render()` method afterward.<keep>*<keep>* @param {number} column Visual column index.<keep>*/\n",
      "\n",
      "### Review Comment:\n",
      "<keep>* Freezes the specified column (i.e. adds it to fixed columns).<keep>*<keep>* @param {number} column Visual column index.<keep>*/\n",
      "\n",
      "### Context:\n",
      "<keep>* @param {number} column Visual column index.<keep>*/\n",
      "\n",
      "### H\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>private void RunGetRequestedDataOtherSampler(Activity activity)<keep>{<keep>ActivityContext parentContext;<del>if (string.IsNullOrEmpty(activity.ParentId))<add>if (string.IsNullOrEmpty(activity.ParentId) || activity.ParentSpanId.ToHexString().Equals(\"0000000000000000\"))<keep>{<keep>parentContext = default;<keep>}\n",
      "\n",
      "### Review Comment:\n",
      "<keep>The code is not clear.\n",
      "\n",
      "### Explanation:\n",
      "The code is not clear.\n",
      "\n",
      "### Diff Hunk:\n",
      "<keep>@@ -1,10 +1,10 @@<keep>private void RunGetRequestedDataOtherSampler(Activity activity)<keep>{\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>updates = append(updates, u...)<keep>}<keep>if len(updates) == 0 {<del>w.logger.Info(\"no image to be updated\")<add>w.logger.Info(\"no image to be updated\",<add>zap.String(\"image-provider\", provider.Name()),<add>)<keep>continue<keep>}<keep>if err := update(updates); err != nil {<del>w.logger.Error(\"failed to update image\", zap.Error(err))<add>w.logger.Error(\"failed to update image\", zap.String(\"image-provider\",<add>provider.Name()),<add>zap.Error(err),<add>)<keep>continue<keep>}<keep>}\n",
      "\n",
      "### Review Comment:\n",
      "<del>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No review comment<add>No\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>/*exported DqElement */<keep>function truncate(str, maxLength) {<del>'use strict';<del><keep>maxLength = maxLength || 300;<keep>if (str.length > maxLength) {\n",
      "\n",
      "### Review Comment:\n",
      "<keep>/*exported DqElement */<keep>function truncate(str, maxLength) {<del>'use strict';<del><keep>maxLength = maxLength || 300;<keep>if (str.length > maxLength) {\n",
      "\n",
      "### Explanation:\n",
      "The code change\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>remote_metadata = check._get_remote_metadata()<keep># TODO: investigate whether encoding ' ' as '%20' makes sense<del>self.assertEqual(check.cache_control, 'public,%20max-age=500')<add>self.assertIn(<add>check.cache_control,<add>('public,%20max-age=500', 'public, max-age=500')<add>)<keep>self.assertEqual(remote_metadata['cache-control'], 'public,%20max-age=500')<keep>self.assertEqual(check.get_metadata('test-plus'), 'A plus (+)')<keep>self.assertEqual(check.content_disposition, 'filename=Sch%C3%B6ne%20Zeit.txt')\n",
      "\n",
      "### Review Comment:\n",
      "<keep># TODO: investigate whether encoding ' ' as '%20' makes sense<del># TODO: investigate whether encoding ' ' as '%20' makes sense<add># TODO: investigate whether encoding ' ' as '%20' makes sense<add># TODO: investigate whether encoding ' ' as '%20'\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>void SetUp() override { QueryTestBase::SetUp(); }<keep>};<del>#define DEDUP_RESUTL_CHECK(inputName, outputName, sentence, expected)                   \\<add>#define DEDUP_RESULT_CHECK(inputName, outputName, sentence, expected)                   \\<keep>do {                                                                                  \\<keep>qctx_->symTable()->newVariable(outputName);                                         \\<keep>auto yieldSentence = getYieldSentence(sentence, qctx_.get());                       \\\n",
      "\n",
      "### Review Comment:\n",
      "<keep>The code is not clear. Please add comments to explain the code.\n",
      "\n",
      "### Explanation:\n",
      "The code is not clear. Please add comments to explain the code.\n",
      "\n",
      "### Code Change:\n",
      "<keep>void SetUp() override { QueryTestBase::SetUp(); }<keep>};<\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>+ \" LEFT JOIN \"<keep>+ \" executors et ON ex.executor_id = et.id\"<keep>+ \" WHERE ex.status NOT IN (\"<add>+ Status.EXECUTION_STOPPED.getNumVal() + \", \"<keep>+ Status.SUCCEEDED.getNumVal() + \", \"<keep>+ Status.KILLED.getNumVal() + \", \"<keep>+ Status.FAILED.getNumVal() + \")\";\n",
      "\n",
      "### Review Comment:\n",
      "<add>+ \" LEFT JOIN \" + \"executors et ON ex.executor_id = et.id\" + \" WHERE ex.status NOT IN (\" + Status.EXECUTION_STOPPED.getNumVal() + \", \" + Status.SUCCEEDED.getNumVal() + \", \"\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>}<keep>void wlr_input_device_destroy(struct wlr_input_device *dev) {<del>if (!dev) return;<add>if (!dev) {<add>return;<add>}<add><keep>if (dev->_device) {<keep>switch (dev->type) {<keep>case WLR_INPUT_DEVICE_KEYBOARD:\n",
      "\n",
      "### Review Comment:\n",
      "<add>// TODO: Implement this function<add>\n",
      "\n",
      "### Explanation:\n",
      "The code change is a deletion of a line. The code change is a deletion of a line. The code change is a deletion of a line. The code change is a deletion of a line. The code change is\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>type taskDeployer interface {<keep>DeployTask(input *deploy.CreateTaskResourcesInput, opts ...cloudformation.StackOption) error<add>DeleteTask(task deploy.TaskStackInfo) error<keep>}<keep>type taskRunner interface {\n",
      "\n",
      "### Review Comment:\n",
      "<keep>DeleteTask(task deploy.TaskStackInfo) error<add>DeleteTask(task deploy.TaskStackInfo) error<keep>}<keep>type taskDeployer interface {<keep>DeployTask(input *deploy.CreateTaskResourcesInput, opts ...cloudformation.StackOption) error<add>\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>{'name': 'cloudsql_acl', 'enabled': False},<keep>{'name': 'iam_policy', 'enabled': True}<keep>]}<add><add>NONEXIST_ENABLED = {'scanners': [<add>{'name': 'bigquery', 'enabled': False},<add>{'name': 'bucket_acl', 'enabled': True},<add>{'name': 'cloudsql_acl', 'enabled': False},<add>{'name': 'non_exist_scanner', 'enabled': True}<add>]}<add><add>ALL_EXIST = {'scanners': [<add>{'name': 'bigquery', 'enabled': True},<add>{'name': 'bucket_acl', 'enabled': True},<add>{'name': 'cloudsql_acl', 'enabled': True},<add>{'name': 'iam_policy', 'enabled': True}<add>]}\n",
      "\n",
      "### Review Comment:\n",
      "<add>\n",
      "\n",
      "### Explanation:\n",
      "<add>\n",
      "\n",
      "### Hint:\n",
      "<add>\n",
      "\n",
      "### Solution:\n",
      "<add>\n",
      "\n",
      "### Answer:\n",
      "<add>\n",
      "\n",
      "### Code Change:\n",
      "<add>\n",
      "\n",
      "### Review Comment:\n",
      "<add>\n",
      "\n",
      "\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>switch (from.typeId()) {<keep>case INTEGER:<del>return to == Types.LongType.get();<add>return to.equals(Types.LongType.get());<keep>case FLOAT:<del>return to == Types.DoubleType.get();<add>return to.equals(Types.DoubleType.get());<keep>case DECIMAL:<keep>Types.DecimalType fromDecimal = (Types.DecimalType) from;\n",
      "\n",
      "### Review Comment:\n",
      "<keep>switch (from.typeId()) {<keep>case INTEGER:<del>return to == Types.LongType.get();<add>return to.equals(Types.LongType.get());<keep>case FLOAT:<del>return to == Types.DoubleType.get();<add\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>@Override<keep>public String getFullNameFor(TypeModel type) {<del>// TODO(andrealin): Remove this hack when null response types are implemented.<del>if (type == null) {<del>return \"nullFullName\";<add>if (type.isEmptyType()) {<add>return \"java.lang.Void\";<keep>}<keep>if (type instanceof DiscoveryRequestType) {<keep>Method method = ((DiscoveryRequestType) type).parentMethod().getDiscoMethod();\n",
      "\n",
      "### Review Comment:\n",
      "<keep>// TODO(andrealin): Remove this hack when null response types are implemented.<keep>if (type == null) {<keep>return \"nullFullName\";<add>if (type.isEmptyType()) {<add>return \"java.lang.Void\";<keep>}<keep>if (\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>public function __construct(Request $request)<keep>{<del>$this->locale = $request->getSession()->getLang()->getLocale();<add>if($request->getSession() != null)<add>$this->locale = $request->getSession()->getLang()->getLocale();<add>else<add>$this->locale = Lang::getDefaultLanguage()->getLocale();<keep>}<keep>public function guessTinyMCELanguage($params, \\Smarty_Internal_Template $template)\n",
      "\n",
      "### Review Comment:\n",
      "<keep>public function __construct(Request $request)<keep>{<del>$this->locale = $request->getSession()->getLang()->getLocale();<add>if($request->getSession() != null)<add>$this->locale = $request->getSession()->getLang()->getLoc\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>DECLARE_string(u);<keep>DECLARE_string(p);<add>DEFINE_bool(enable_history, false, \"Whether to force saving the command history\");<keep>namespace nebula {<keep>namespace graph {\n",
      "\n",
      "### Review Comment:\n",
      "<add>The flag enable_history is added to the code.\n",
      "\n",
      "### Hint:\n",
      "<add>The flag enable_history is added to the code.\n",
      "\n",
      "### Example:\n",
      "<add>The flag enable_history is added to the code.\n",
      "\n",
      "### Input Format:\n",
      "<add>The flag\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>*/<keep>if (!isType (token, TOKEN_OPEN_PAREN))<keep>readToken (token);<add>if (!isType (token, TOKEN_OPEN_PAREN))<add>return;<del>Assert (isType (token, TOKEN_OPEN_PAREN));<keep>do<keep>{<keep>if (isType (token, TOKEN_COMMA) ||\n",
      "\n",
      "### Review Comment:\n",
      "<keep>/*<keep>*/<keep>if (!isType (token, TOKEN_OPEN_PAREN))<keep>readToken (token);<add>/*<add>*/<add>if (!isType (token, TOKEN_OPEN_PAREN))<add>return;<\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>from mitmproxy import log<keep>from mitmproxy import version<keep>from mitmproxy import optmanager<add>from mitmproxy import options<keep>import mitmproxy.tools.web.master # noqa<add>CONFIG_PATH = os.path.join(options.CA_DIR, 'config.yaml')<add><keep>def flow_to_json(flow: mitmproxy.flow.Flow) -> dict:<keep>\"\"\"\n",
      "\n",
      "### Review Comment:\n",
      "<add>The code is not well-documented.\n",
      "\n",
      "### Explanation:\n",
      "The code is not well-documented.\n",
      "\n",
      "### Hint:\n",
      "<add>The code is not well-documented.\n",
      "\n",
      "### Solution:\n",
      "<add>The code is not well-documented.\n",
      "\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>#<keep># @since 2.0.0.rc.7<keep>def process_attribute(name, value)<del>if store_as = aliased_fields.invert[name.to_s]<add>responds = respond_to?(\"#{name}=\")<add>if !responds && store_as = aliased_fields.invert[name.to_s]<keep>name = store_as<keep>end<keep>responds = respond_to?(\"#{name}=\")\n",
      "\n",
      "### Review Comment:\n",
      "<keep>#<keep># @since 2.0.0.rc.7<keep>def process_attribute(name, value)<del>if store_as = aliased_fields.invert[name.to_s]<add>responds = respond_to?(\"#{name}=\")\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>workingDir = workspaceRootDirectory;<keep>}<del>GeneralCommandLine commandLine = new GeneralCommandLine(runner.executableToDebug.getPath());<add>GeneralCommandLine commandLine = new GeneralCommandLine(runner.executableToDebug.getPath())<add>.withWorkDirectory(workingDir);<keep>commandLine.addParameters(handlerState.getExeFlagsState().getFlagsForExternalProcesses());<keep>commandLine.addParameters(handlerState.getTestArgs());\n",
      "\n",
      "### Review Comment:\n",
      "<add>The working directory is set to the root directory of the project.\n",
      "\n",
      "### Hint:\n",
      "<add>The working directory is set to the root directory of the project.\n",
      "\n",
      "### Hint:\n",
      "<add>The working directory is set to the root directory of the project.\n",
      "\n",
      "### Hint\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<add>using System;<add>using System.Collections.Generic;<add>using System.Linq;<add>using System.Text;<add>using System.Threading.Tasks;<add>using MvvmCross.Core.Views;<add><add>namespace MvvmCross.Uwp.Attributes<add>{<add>public class MvxPagePresentationAttribute : MvxBasePresentationAttribute<add>{<add>}<add>}\n",
      "\n",
      "### Review Comment:\n",
      "<add>\n",
      "\n",
      "### Explanation:\n",
      "<add>\n",
      "\n",
      "### Diff Hunk:\n",
      "<add>\n",
      "\n",
      "### Context:\n",
      "<add>\n",
      "\n",
      "### Your Suggested Review Comment:\n",
      "<add>\n",
      "\n",
      "### Explanation:\n",
      "<add>\n",
      "\n",
      "### D\n",
      "You are a powerful code reviewer model. Your job is to suggest review comment in natural language. You are given a question, and context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\n",
      "\n",
      "### Question:\n",
      "What would be your suggested review comment?\n",
      "\n",
      "### Code Change:\n",
      "<keep>result.stdout << std_out<keep>result.stderr << std_err<keep>result.exit_code = status.exitstatus<add>@logger.info(result.stdout)<add>@logger.info(result.stderr)<keep>end<keep>rescue => e<keep>result.stderr << e.inspect<add>@logger.info(result.stderr)<keep>result.exit_code = 1<keep>end\n",
      "\n",
      "### Review Comment:\n",
      "The code change is a bit confusing. I would suggest to use a more descriptive variable name.\n",
      "\n",
      "### Hint:\n",
      "You can use the following command to get the diff hunk:\n",
      "\n",
      "```\n",
      "git diff --no-index --no-prefix --word-diff=color --word-diff-regex='[[:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m tokenizer(eval_prompts, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 19\u001B[0m     generated \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m70\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m gen \u001B[38;5;129;01min\u001B[39;00m generated:\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(gen, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/peft/peft_model.py:1140\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.generate\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mgeneration_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[1;32m   1139\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1140\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1141\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m   1142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/transformers/generation/utils.py:1478\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massisted_decoding(\n\u001B[1;32m   1462\u001B[0m         input_ids,\n\u001B[1;32m   1463\u001B[0m         candidate_generator\u001B[38;5;241m=\u001B[39mcandidate_generator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1474\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1475\u001B[0m     )\n\u001B[1;32m   1476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGREEDY_SEARCH:\n\u001B[1;32m   1477\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n\u001B[0;32m-> 1478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgreedy_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1479\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1483\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1484\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1485\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1486\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1488\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1491\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mCONTRASTIVE_SEARCH:\n\u001B[1;32m   1492\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/transformers/generation/utils.py:2315\u001B[0m, in \u001B[0;36mGenerationMixin.greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   2312\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   2314\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 2315\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2316\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2317\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2318\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2319\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2320\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2322\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[1;32m   2323\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/accelerate/hooks.py:165\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 165\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m/media/quadro/NVME/afnan/conda/codellama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1207\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1206\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n\u001B[0;32m-> 1207\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1209\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1211\u001B[0m     \u001B[38;5;66;03m# Shift so that tokens < n predict n\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Assuming `test_df` is your DataFrame containing test data\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Modify the prompt generation and inference to run on batches\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "total_data_points = 500  # Number of data points to process\n",
    "\n",
    "for i in range(0, total_data_points, batch_size):\n",
    "    batch_df = test_df.iloc[i:i+batch_size]\n",
    "    eval_prompts = []\n",
    "\n",
    "    for j in range(len(batch_df)):\n",
    "        eval_prompt = prompt1 + batch_df.iloc[j][\"diff\"] + prompt2\n",
    "        eval_prompts.append(eval_prompt)\n",
    "\n",
    "    model_inputs = tokenizer(eval_prompts, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(**model_inputs, max_new_tokens=70)\n",
    "\n",
    "    for gen in generated:\n",
    "        print(tokenizer.decode(gen, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codellama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
